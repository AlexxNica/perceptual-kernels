{"name":"perceptual-kernels","tagline":"Data and source code for the first phase of the perceptual kernels study. ","body":"Learning Perceptual Kernels for</br> Visualization Design\r\n===================================================\r\n\r\nThis repo contains the results and source code from our crowdsourced experiments to estimate\r\nperceptual kernels for color, shape, size and combinations thereof. What is a perceptual kernel? \r\nIt is a distance matrix derived from aggregate perceptual judgments. In its basic form, a perceptual kernel \r\ncontains pairwise perceptual dissimilarity values for a specific set of perceptual stimuli---we refer \r\nto this set as a palette. In our study, we estimate perceptual kernels for the following six palettes. \r\n\r\n<img width=\"512\" align=\"middle\" src=https://github.com/uwdata/perceptual-kernels/blob/master/doc/imgs/allpalettes.svg?raw=true>\r\n\r\nThere can be several alternative ways for experimentally constructing perceptual kernels. \r\nFor example, we construct perceptual kernels from subjective similarity judgments. \r\nPsychology literature offers several task types for similarity judgments. \r\nHow to choose one? What is the most effective judgment task in the context of perceptual \r\nkernels? So, understanding the trade-offs between different designs of judgment tasks is important. \r\nWe estimate five perceptual kernels for each of the palettes above using the five different \r\njudgment tasks below---links show the task interfaces of the shape palette (refresh your page if you a garbled image). \r\n+ [Pairwise rating on 5-Point Scale (L5)](https://rawgit.com/uwdata/perceptual-kernels/master/exp/shape/l5/shape-l5.html)\r\n+ [Pairwise rating on 9-Point scale (L9)](https://rawgit.com/uwdata/perceptual-kernels/master/exp/shape/l9/shape-l9.html)\r\n+ [Triplet ranking with matching (Tm)](https://rawgit.com/uwdata/perceptual-kernels/master/exp/shape/tm/shape-tm.html)\r\n+ [Triplet ranking with discrimination (Td)](https://rawgit.com/uwdata/perceptual-kernels/master/exp/shape/td/shape-td.html)\r\n+ [Spatial arrangement (SA)](https://rawgit.com/uwdata/perceptual-kernels/master/exp/shape/sa/shape-sa.html)\r\n\r\n\r\nHow to use the data and source code in this repo? \r\n------------------------------------------------\r\nThere are several ways to do that. \r\n\r\nFirst, you can  directly access the final perceptual kernels and use them for your own purposes, \r\nresearch or otherwise. You will see thirty kernels in [data/kernels/](https://github.com/uwdata/perceptual-kernels/tree/master/data/kernels) folder. These are symmetric, normalized matrices, stored as comma-seperated text files. File names reveal the variable and judgment task types used. For example, [color-sa.txt](https://github.com/uwdata/perceptual-kernels/tree/master/data/kernels/color-sa.txt) is the perceptual kernel for the color palette and was obtained using  spatial arragement. \r\n\r\nSecond, you can reproduce and extend our experiments using the source code provided. \r\nOr you can just copy them to bootstrap your own new experiments. Each experiment is designed to \r\nbe as self-contained as possible. For example, if you would like to see the experiment \r\nsetup produced color-sa.txt, you can go to [exp/color/sa/](https://github.com/uwdata/perceptual-kernels/tree/master/exp/color/sa) directory. You can check \r\nout the task interface  by opening  [color-sa.html](https://github.com/uwdata/perceptual-kernels/tree/master/exp/color/sa/color-sa.html) in your browser. We recommend \r\nyou go through and perform the task to understand what it does. \r\nIf you want to reproduce this experiment (or other experiments in exp/, for that matter), you need to \r\nfirst install  [Amazon Mechanical Turk Command Line Tools](https://aws.amazon.com/developertools/Amazon-Mechanical-Turk/694) and then set two environment variables: MTURKCLT_HOME, which should point the installation directory for Amazon's command line tools,  and STUDY_HOME , which should be set to the current perceptual-kernels directory. \r\n\r\n\r\nWhat is a perceptual kernel?\r\n----------------------------\r\nPerceptual kernels are distance matrices derived from aggregate perceptual similarity judgments. \r\nHere is an example of a perceptual kernel:\r\n\r\n![](https://github.com/uwdata/perceptual-kernels/blob/master/doc/imgs/tmshape.png?raw=true)\r\n<p>(Left) A crowd-estimated perceptual kernel for a shape palette. Darker entries indicate \r\nperceptually closer (similar) shapes. (Right) A two-dimensional projection of the palette \r\nshapes obtained via [multidimensional scaling](http://en.wikipedia.org/wiki/Multidimensional_scaling) of the perceptual kernel. \r\n\r\nWhat is it useful for? \r\n----------------------\r\nVisualization design benefits from careful consideration of perception,\r\nas different assignments of visual encoding variables such as color, shape and size\r\naffect how viewers interpret data. Perceptual kernels represent perceptual differences between and\r\nwithin visual variables in a reusable form that is directly applicable to\r\nvisualization evaluation and automated design. In other words, perceptual kernels \r\nprovide a useful operational model for incorporating empirical perception data directly \r\ninto visualization design tools.  Please refer to our [draft on perceptual kernels](https://rawgit.com/uwdata/perceptual-kernels/master/doc/perceptual-kernels.pdf) \r\nfor further details. \r\n\r\nHere are few examples of how the kernels can be used. \r\n\r\nAutomatically designing new palettes\r\n------------------------------------ \r\nGiven an estimated perceptual kernel, we can use it to revisit existing palettes. \r\nFor example, we can choose a set of stimuli that maximizes perceptual distance or \r\nconversely minimizes perceptual similarity according to the kernel.\r\nThe following shows the n most discriminable subsets of the shape, size, and color variables. \r\n(We include size for completeness,  though in practice this palette is better suited to quantitative, \r\nrather than categorical, data.)  To compute a subset with n elements, we first initialize the set with \r\nthe variable pair that  has the highest perceptual distance. We then add new elements to this set, by finding the variable \r\nwhose minimum distance to the existing subset is the maximum (i.e., the  [Hausdorff distance](http://en.wikipedia.org/wiki/Hausdorff_distance) between two point sets).\r\n\r\n<img width=\"600\"src=https://github.com/uwdata/perceptual-kernels/blob/master/doc/imgs/tmnewpalette.svg?raw=true>\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}